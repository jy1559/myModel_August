{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2978efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dataset_folder = '/home/jy1559/Mar2025_Module/Datasets'\n",
    "dataset_name = 'Retail_Rocket'\n",
    "INTER_FILE = os.path.join(dataset_folder, dataset_name, \"interactions_revised.json\")\n",
    "CAND_FILE = os.path.join(dataset_folder, dataset_name, \"candidate_sets_revised.npz\")\n",
    "ITEM_EMB = os.path.join(dataset_folder, dataset_name, \"item_embedding_normalized_revised.pickle\")\n",
    "ART_EMB = os.path.join(dataset_folder, dataset_name, \"articles_embeddings.pickle\")\n",
    "\n",
    "# dense 버전 파일명\n",
    "INTER_OUT = os.path.join(dataset_folder, dataset_name, \"interactions_dense.json\")\n",
    "CAND_OUT = os.path.join(dataset_folder, dataset_name, \"candidate_sets_dense.npz\")\n",
    "ITEM_OUT = os.path.join(dataset_folder, dataset_name, \"item_embedding_dense.pickle\")\n",
    "ART_OUT = os.path.join(dataset_folder, dataset_name, \"articles_embeddings_dense.pickle\")\n",
    "MAP_FILE = os.path.join(dataset_folder, dataset_name, \"item_id_mapping.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6984acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique item count: 100,822\n",
      ">> mapping saved : /home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/item_id_mapping.pkl\n"
     ]
    }
   ],
   "source": [
    "# 1) 모든 item id 수집\n",
    "with open(INTER_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "item_set = set()\n",
    "for sessions in data_json[\"data\"].values():\n",
    "    for sess in sessions:\n",
    "        item_set.update(int(t[0]) for t in sess)\n",
    "\n",
    "print(f\"Unique item count: {len(item_set):,}\")\n",
    "\n",
    "# 2) dense id (0=PAD, 1~N)\n",
    "dense_ids = {orig: idx+1 for idx, orig in enumerate(sorted(item_set))}\n",
    "reverse   = {v:k for k,v in dense_ids.items()}\n",
    "\n",
    "# 3) 저장\n",
    "with open(MAP_FILE, \"wb\") as f:\n",
    "    pickle.dump({\"id2idx\": dense_ids, \"idx2id\": reverse}, f)\n",
    "print(\">> mapping saved :\", MAP_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851a869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> saved : /home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/interactions_dense.json\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "dense_json = deepcopy(data_json)\n",
    "for user, sessions in dense_json[\"data\"].items():\n",
    "    for sess in sessions:\n",
    "        for trip in sess:\n",
    "            trip[0] = dense_ids[int(trip[0])]             # id 재매핑\n",
    "\n",
    "with open(INTER_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dense_json, f)\n",
    "print(\">> saved :\", INTER_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e61cbfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remap: 100%|██████████| 100822/100822 [00:02<00:00, 36173.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> saved : /home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/candidate_sets_dense.npz (100823, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pickle, tqdm, torch\n",
    "\n",
    "K = 128                      # 후보 크기 (원본과 동일)\n",
    "N_dense = len(dense_ids)     # 46 033\n",
    "pad_row = np.zeros(K, dtype=np.int32)\n",
    "\n",
    "# 1) 원본 후보셋 로드\n",
    "old = np.load(CAND_FILE)[\"candidate_tensor\"]   # shape [364047, K]\n",
    "\n",
    "# 2) 새 행렬  [N_dense+1, K]\n",
    "new = np.zeros((N_dense+1, K), dtype=np.int32)  # 0행 PAD\n",
    "\n",
    "# 3) 각 dense_id 행 채우기\n",
    "for orig_id, d_id in tqdm.tqdm(dense_ids.items(), desc=\"remap\"):\n",
    "    # 원본 후보 리스트\n",
    "    cand_row = old[orig_id]                    # [K] orig ids\n",
    "    # orig→dense 매핑 (PAD=0)\n",
    "    new[d_id] = np.vectorize(lambda x: dense_ids.get(int(x), 0),\n",
    "                             otypes=[np.int32])(cand_row)\n",
    "\n",
    "# 4) 저장\n",
    "np.savez_compressed(CAND_OUT, candidate_tensor=new)\n",
    "print(\">> saved :\", CAND_OUT, new.shape)   # (46 034, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e325c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> saved : /home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/item_embedding_dense.pickle torch.Size([100823, 384])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with open(ITEM_EMB, \"rb\") as f:\n",
    "    emb_dict = pickle.load(f)   # {'embedding_tensor': Tensor[N+1,384]}\n",
    "\n",
    "orig_tensor = emb_dict[\"embedding_tensor\"]\n",
    "D = orig_tensor.shape[1]\n",
    "dense_tensor = torch.zeros(len(dense_ids)+1, D)   # 0행 PAD 0벡터\n",
    "\n",
    "for orig_id, dense_id in dense_ids.items():\n",
    "    dense_tensor[dense_id] = orig_tensor[orig_id]\n",
    "\n",
    "pickle.dump({\"embedding_tensor\": dense_tensor}, open(ITEM_OUT,\"wb\"))\n",
    "print(\">> saved :\", ITEM_OUT, dense_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b45a10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/articles_embeddings.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mART_EMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     art \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)           \u001b[38;5;66;03m# ndarray  shape [N+1, 384]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m art_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(art, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)   \u001b[38;5;66;03m# 🔹 numpy → torch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/first/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jy1559/Mar2025_Module/Datasets/Retail_Rocket/articles_embeddings.pickle'"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, pickle\n",
    "\n",
    "with open(ART_EMB, \"rb\") as f:\n",
    "    art = pickle.load(f)           # ndarray  shape [N+1, 384]\n",
    "\n",
    "art_t = torch.as_tensor(art, dtype=torch.float32)   # 🔹 numpy → torch\n",
    "art_dense = torch.zeros(len(dense_ids)+1, art_t.shape[1])\n",
    "\n",
    "for orig_id, dense_id in dense_ids.items():\n",
    "    art_dense[dense_id] = art_t[orig_id]            # 🔹 torch ← torch\n",
    "\n",
    "pickle.dump({\"embedding_tensor\": art_dense}, open(ART_OUT, \"wb\"))\n",
    "print(\">> saved :\", ART_OUT, art_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ PAD row (0~3) tensor([0., 0., 0., 0.])\n",
      "▶ random id check  orig 7 → dense 1\n",
      "   vec equal?  False\n",
      "\n",
      "item_embedding_dense.pickle : torch.Size([100823, 384])\n",
      "candidate_sets_dense.npz     : (100823, 128)\n",
      "\n",
      "Mapping sizes  id2idx: 100822 | idx2id: 100822\n"
     ]
    }
   ],
   "source": [
    "import pickle, torch\n",
    "\n",
    "# ── 1. numpy → torch 변환 후 allclose 검증 ─────────────────────────────\n",
    "rand_orig = next(iter(dense_ids))\n",
    "rand_dense = dense_ids[rand_orig]\n",
    "\n",
    "art_np   = art[rand_orig]                     # numpy 1D\n",
    "art_t    = torch.as_tensor(art_np, dtype=torch.float32)\n",
    "is_equal = torch.allclose(art_t, art_dense[rand_dense])\n",
    "\n",
    "print(\"▶ PAD row (0~3)\", art_dense[0,:4])\n",
    "print(f\"▶ random id check  orig {rand_orig} → dense {rand_dense}\")\n",
    "print(\"   vec equal? \", is_equal)\n",
    "\n",
    "# ── 2. 새로 저장된 파일들의 차원 확인 ────────────────────────────────\n",
    "with open(ITEM_OUT, \"rb\") as f:\n",
    "    item_emb_dense = pickle.load(f)[\"embedding_tensor\"]\n",
    "print(\"\\nitem_embedding_dense.pickle :\", item_emb_dense.shape)   # (N_dense+1, 384)\n",
    "\n",
    "cand_dense = np.load(CAND_OUT)[\"candidate_tensor\"]\n",
    "print(\"candidate_sets_dense.npz     :\", cand_dense.shape)        # (N_dense+1, K)\n",
    "\n",
    "with open(ART_OUT, \"rb\") as f:\n",
    "    art_dense_loaded = pickle.load(f)[\"embedding_tensor\"]\n",
    "print(\"articles_embeddings_dense    :\", art_dense_loaded.shape)  # (N_dense+1, 384)\n",
    "\n",
    "# ── 3. id-매핑 dict 크기 확인 ────────────────────────────────────────\n",
    "with open(MAP_FILE, \"rb\") as f:\n",
    "    maps = pickle.load(f)\n",
    "print(\"\\nMapping sizes  id2idx:\", len(maps['id2idx']), \"| idx2id:\", len(maps['idx2id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab686b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4b99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
